\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}

\usepackage{minted}

\usepackage{hyperref}

\usepackage{float}

\usepackage{tikz}

\usepackage{amsmath}

\usepackage{pgfplots}
\pgfplotsset{compat=newest, scaled y ticks=false}

\begin{document}

\title{
    \textbf{Arrays and performance}
}
\author{Hannes Mann}
\date{2022-08-31}

\maketitle

\section*{Introduction}

This is a report for the introductory assignment in the course ID1021 Algorithms and Data Structures. In this assignment, the efficiency of three simple operations operating on arrays should be explored.

The focus is not on the implementation of these operations but rather on measuring their execution time and exploring how the execution time grows as the algorithms are executed with larger values.

\section*{Method}

The implementation of the three operations was done in C++, but no containers or algorithms from the C++ standard library were used. The code could easily be ported to C but uses C++ to make use of the utilities in the standard library, such as platform-independent time measurement with the {\tt <chrono>} header.

Compilation was done with GCC 12.2.0 and utilizes the {\tt -O0} parameter to ensure the compiler doesn't optimize away any part of the code.

The source code can be found at: \href{https://pastebin.com/3hpMwuMz}{https://pastebin.com/3hpMwuMz}

\section*{Results}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{n} & \textbf{access} & \textbf{search} & \textbf{duplicates} \\
\hline
  100 & 0.87 ns & 94 $\mu$s & 91 $\mu$s \\
  500 & 0.85 ns & 431 $\mu$s & 1900 $\mu$s \\
  1000 & 0.85 ns & 952 $\mu$s & 7300 $\mu$s \\
  2000 & 0.86 ns & 1940 $\mu$s & 29100 $\mu$s \\
  3000 & 0.86 ns & 2400 $\mu$s & 66100 $\mu$s \\
  4000 & 0.86 ns & 3160 $\mu$s & 119000 $\mu$s \\
  5000 & 0.90 ns & 3930 $\mu$s & 189000 $\mu$s \\
\hline
\end{tabular}
\caption{Execution time for the different operations on an array of size $n$.}
\label{tab:table1}
\end{table}

\section*{Discussion}

Analysing the operations requires studying what happens to their execution time as the size of the array increases.
For the first operation, where a random element in an array is accessed, it's obvious from the measured values that the execution time
of a single access does not change when the array grows (outside of noise present in the measurement). This means random access is an $O(1)$ operation.

\begin{table}[H]
\centering
\begin{tikzpicture}
\begin{axis}
[
	xlabel={n},
	ylabel={t},
	grid,
	xmin=100,
	xmax=5000,
	ymin=0,
	ymax=20,
	y label style={rotate=-90}
]
\addplot[red, no marks, densely dashed] table[col sep=comma]{plot1.csv};
\addlegendentry{random access}
\end{axis}
\end{tikzpicture}
\end{table}

The second and third operation involves searching through an array for a number of randomly picked keys.
Both use the same algorithm, but the ''search'' operation looks for 10 different random keys ($m=10$),
while the ''duplicates'' operation grows the number of random keys proportionally to the size of the array ($m=n$).
Plotting gives a clear picture of the effect the number of random keys has on the execution time.

\begin{table}[H]
\centering
\begin{tikzpicture}
\begin{axis}
[
	xlabel={n},
	ylabel={t},
	grid,
	xmin=100,
	xmax=5000,
	ymin=0,
	ymax=4000,
	y label style={rotate=-90}
]
\addplot[red, no marks, densely dashed] table[col sep=comma]{plot2.csv};
\addlegendentry{search, $m=100$}
\end{axis}
\end{tikzpicture}
\end{table}

Searching with a constant amount of random keys shows that the time grows linearly with the size of the array. This is an $O(n)$ operation.

\begin{table}[H]
\centering
\begin{tikzpicture}
\begin{axis}
[
	xlabel={n},
	ylabel={t},
	grid,
	xmin=100,
	xmax=5000,
	ymin=0,
	ymax=200000,
	y label style={rotate=-90}
]
\addplot[red, no marks, densely dashed] table[col sep=comma]{plot3.csv};
\addlegendentry{duplicates, $m=n$}
\end{axis}
\end{tikzpicture}
\end{table}

Searching with an increasing amount of random keys shows that the time grows linearly with the size of the array. This is an $O(n^2)$ operation.

This makes a lot of sense when you consider that all the search algorithm is doing is iterating through two arrays.
Iterating through one array is an $O(n)$ operation, so iterating through two arrays is an $O(n)*O(m)$ operation. When $n=m$ this becomes $O(n)*O(n) = O(n^2)$ and when $n=100$ this becomes $O(n) * O(1) = O(n)$.

\subsection*{Estimating the execution time}

\begin{itemize}
  \item \textbf{Random access}: Averaging the measurements gives: $t\approx0.86$ $ns$.
  \item \textbf{Searching through an array with 100 keys}: Dividing by $n$ and averaging gives: $t\approx 0.87n$ $\mu s$.
  \item \textbf{Searching for duplicates}: Solving the equation $t = (Cn)^2$ where $C$ is the constant time for two reads gives: $\sqrt{t}\approx 0.087n$ $\Rightarrow$ $t\approx 0.0076n^2$ $\mu s$.
\end{itemize}

As can be seen, even though the algorithms perform different operations for every loop iteration (random access performs one read and search performs two reads), the time taken for each loop iteration is almost identical.
The most likely explanation for this is that modern processors execute multiple instructions in one cycle, so one read takes the same time as two reads (while in theory it should take twice as long).

\subsection*{Estimating how many duplicates can be found in one hour}

The equation $t\approx 0.0076n^2$ $\mu s$ is specified in microseconds, and the number of microseconds in an hour is 36000000000 $\mu s$.

\begin{equation*}
0.0076n^2 \approx 36000000000
\end{equation*}
\begin{equation*}
0.087n \approx \sqrt{36000000000} \approx 189737
\end{equation*}
\begin{equation*}
n \approx 2180885
\end{equation*}

On the computer the program was tested on the search algorithm should be able to find around 2180000 duplicates in one hour.

\end{document}
